# ACONTRA

The ACONTRA-LAMA platform was developed as part of the project &quot;The Affective Construction Of National Temporalities in Austrian Post-War Radio&quot; (ACONTRA), a collaborative project between the University of Music and Performing Arts, the University of Vienna, and the House of Austrian History. ACONTRA focuses on the role of radio as a mass medium in the process of constructing national consciousness during the early years of the Second Republic (1945-1955).
Auditory primary sources for this period have so far been poorly catalogued or not at all: The collections are highly fragmented and so diverse in structure and content that they must first be made accessible for (scientific) processing. Through the further development of the annotation software LAMA (Linking Annotations for Media Analysis), which was conceived in the predecessor project Telling Sounds, existing collections are recorded, merged, and finally, connections between the documents are established at both the production and content levels. For this purpose, a corpus had to be researched and defined from the existing collections. The existing metadata of the documents were standardised. Additionally, the documents were annotated to varying degrees with both descriptive and interpretative annotations. This enables the preparation of the material with regard to specific research questions.

## How to use the platform:
The ACONTRA-LAMA may be used in three different ways:
1. There are various search functions to search the available documents according to specific parameters: there are filter and search functions in the &quot;Clip List&quot; to find specific clips; a quote search to find specific words and phrases in quotes; and the more complex &quot;Query Function&quot; to discover similarities between the clips.
2. The &quot;Detailed Analysis&quot;-tool may be used to break down and annotate the sound, music and speech layers of our audio documents according to your own research questions. If you are interested in using the &quot;Detailed Analysis&quot;-function, you will need access-data, so please contact: Birgit Haberpeuntner ([birgit.haberpeuntner@univie.ac.at](mailto:birgit.haberpeuntner@univie.ac.at)) and/or Elias Berner ([elias.berner@oeaw.ac.at](mailto:elias.berner@oeaw.ac.at)). We also offer a brief orientation re: how to best use this tool!
3. If you are interested in the source code, e.g., to index and research other archival collections, you will find it here: [https://github.com/tellingsounds/acontralama](https://github.com/tellingsounds/acontralama). If you have any questions please contact Julia Jaklin ([julia.jaklin@tuwien.ac.at](mailto:julia.jaklin@tuwien.ac.at)).


## Radio landscape and archival situation in Austria, 1945&ndash;1955
The outlines of the Austrian post-war radio landscape were already drawn in 1945 through the Allied invasion routes: the British _Sendergruppe Alpenland_, with transmitters in Graz and Klagenfurt, was founded on August 31, 1945 after the British army took over Styria from the Soviets; in July 1945, after Tyrol had also become part of the French occupation zone, _Radio Vorarlberg_ and _Radio Innsbruck_ broadcast together under French administration as _Sendergruppe West_; in June 1945, the US-American troops announced the establishment of a radio station in Salzburg under the name _Rot-Weiß-Rot_, and also initiated the construction of stations in Linz and Vienna. The Soviets had no plans to operate their own German-language station: in April 1945, they took over the city of Vienna and with it, the building that housed the broadcasting corporation at Argentinierstraße, which had been built in the 1930s for the _RAVAG_ (Radio Verkehrs AG) and became the headquarters of the Reichsender Wien from 1938. They contented themselves with monitoring RAVAG's immediately resumed program, as well as creating a regular series of broadcasts entitled _Russische Stunde_. Although _RAVAG_ had to comply with the censorship regulations of the Soviets, it served as the mouthpiece of the provisional government under Karl Renner.
The available archival holdings of radio documents from 1945&ndash;1955 are as fragmented as the radio landscape of the post-war period, and they consequently spread across different archives. Holdings of various broadcasters and broadcasting groups may be found in the ORF Archives, the _Österreichische Mediathek_ (Austrian Media Library), the _DokuFunk_ (Radio Documentation Archive) and the MDW (University of Music and Performing Arts in Vienna). However, most of the documents available there are recordings from the two institutions with the widest reach, i.e. _RAVAG_ and _Rot-Weiß-Rot_. Occasionally, documents from _Sendergruppe West_ or _Sendergruppe Alpenland_ have also found their way into these archives, but in order to supplement these holdings, the regional archives of what has later become the provincial studios would also have to be researched. In our project, we concentrate on the holdings from 1945&ndash;1955 available in the ORF archives and in the _Mediathek_, mostly from the _RAVAG_ and _Rot-Weiß-Rot_. As our source materials, we defined the tapes that were added to the _UNESCO World Heritage List_ in 2016 under the title &quot;Historical Radio Recordings _RAVAG_ and _Rot-Weiß-Rot_&quot;: There, the number of tapes from the ORF archive is listed as 620, and from the _Mediathek_ as 215.


## What really happened: The data sets we have indexed
However, the holdings from the UNESCO collections also turned out to be porous. For one, it was no longer possible to reconstruct in detail which 620 documents from the ORF archives were actually picked to be included in the UNESCO list. As a team, we therefore decided to expand the corpus at this point, and to include all available documents from the ORF archive for the period 1945&ndash;1955 in our database. Therefore, we did not work with 620 archival documents from the ORF, but with 1695. This resulted in 1856 entries in our database, as we split up collection tapes into individual entries.
On the other hand, the _Rot-Weiß-Rot_ collection of the _Mediathek_ was easy to reconstruct, as it was a self-contained purchase of 213 physically existing broadcasting tapes (two signatures were given out twice, hence the 215 in the UNESCO collection). However, more detailed research revealed that at least 49 of these 213 tapes had to be re-dated to after 1955; thus, they not only lie outside the period under investigation, but they cannot be _Rot-Weiß-Rot_ recordings at all, because the last RWR station was discontinued on July 27, 1955. However, as we have expanded our corpus from the ORF archive to account for additional findings, we did the same thing at the _Mediathek_: In addition to the 164 tapes we were able to attribute to _Rot-Weiß-Rot_ from the UNESCO-collection, we have also included documents that could be found in the _Mediathek_ under the search term &quot;radio&quot; for the period 1945&ndash;1955. During our research, we also became aware that a further 200 or so documents could be found by searching for the corporation &quot;_Rot-Weiß-Rot_&quot; for our period of investigation (1945&ndash;1955), which are not part of the above-mentioned _Rot-Weiß-Rot_ collection and are not concert recordings of the Vienna Symphony Orchestra (which is its own collection with approx. 2000 documents). These approx. 200 we were not able to include in the database due to our limited capacities. We have, however, managed to include a total of 463 entries from the _Mediathek_ in our database: 235 entries for the search term &quot;radio&quot; for the period 1945&ndash;1955, and 220 entries for the _Rot-Weiß-Rot_ collection (collection tapes with different pieces of music were broken down into individual entries).
In addition, we have created separate entries &ndash; in a separate collection category &ndash; for all documents used in the exhibition accompanying our project, in order to optimize the selection and cooperation process. As a result, an additional 30 or so entries have been added to our corpus, which do not fall within the period 1945&ndash;1955.


## The indexing process
The imported documents were indexed with our own metadata in 4 steps in increasing depth.
1. Automated import of title, signature and specified production or creation date
2. Translation of the descriptive texts into machine-readable entities: persons, institutions, locations, topics.
3. Descriptive detailed analysis of the individual levels of a document: speech, sounds, music (for this step, the documents had to be listened to)
4. Interpretations (i.e., a distinct relation) based on the entities created in the descriptive detailed analyses, which can be &quot;merged&quot; into an interpretation.

Steps 1&ndash;2 involved the translation and standardization of the information found in the archives, in accordance with linkable, machine-readable standards (linked open data). These steps were carried out for all imported documents (basic data collection).

Steps 3&ndash;4 were only carried out for a select number of documents (currently 45, as of September 2024). The selection process was guided by our research questions about the connection between nationhood and temporality; by crucial emphases set by the exhibition; and by the collected materials themselves through basic data queries.

## Reservations: A note for users and future projects
- We can, by no means, lay claim to complete coverage of all radio documents available in the above archives from 1945&ndash;1955. There is still a lot to be found, especially in the _Mediathek_, but possibly also in the ORF. In particular, we assume that there is plenty of interesting material recorded by amateurs on private devices, some of which may be hidden in archival holdings at the _Mediathek_ that may not yet have been listened to.
- We have not had the time to listen to all the documents for ourselves. Thus, the preliminary work of the archivists is crucial in finding relevant documents for our detailed analysis, as the quality and quantity of our basic metadata reflects what has already been entered in the archives as descriptions of the documents.
- Distinction between basic metadata annotation and &quot;analytical annotation&quot; / interpretation is nowhere near clear-cut. Tags assigned during basic annotation are, at times, just as subjective or interpretative (e.g. the keyword &quot;Austrianness&quot; requires a concept of what Austrianness is and what the tag refers to in the respective clip).
- Certain annotations have only emerged as relevant during the processing of the material and as research progressed. They have, consequently, been used less frequently and accurately for the previously annotated documents.


## About the application

LAMA is a web application implemented in TypeScript (React, Material UI), with a REST-backend implemented in Python (Bottle), developed in the _Telling Sounds_ project at the mdw Vienna by Julia Jaklin and Peter Provaznik and has been adapted to suite the needs of the _ACONTRA_ project by Julia Jaklin.
All user actions that produce changes are stored in an event log (a SQLite database).
This provides a full history, makes restoring past states or undoing unwanted actions simple, while also providing flexibility regarding changes in data representation.
The data from the event log is fed into a MongoDB server, making the current application state available for querying and retrieval.
This means that the event log is the source of truth for the system, from which the MongoDB representation is created. This can be done with the `reset_from_eventlog.py` script, either restoring the state from the current events database, or providing an exported (`lama.importexport`) XML file.

## Local manual build

### Prerequisites

+ Python 3.8 or newer
+ current Node.js (last version used was v16.15.0)
+ MongoDB 4.4 server running at `localhost:27017`

### Building

+ `cd` to the project root
+ we recommend creating virtualenv: `$ python3 -m venv .venv`
+ activate virtualenv (or use `.venv/bin/python`): `$ . .venv/bin/activate`
+ install python packages: `$ pip install -e backend/`
+ `$ cd frontend/`
+ install npm packages: `$ npm install`
+ compile JavaScript: `$ API_URL="http://localhost:3333" WS_URL="ws://localhost:3333/ws" npm run build:production` (ignore warnings/errors)

### Setup

+ `cd` back to project root: `$ cd ..`
+ create user: `$ python scripts/lamaherder.py createuser admin --privileges a --password Admin123`
+ optionally import data: `$ python scripts/replace_mongo_data.py full.json` (or `PhA.json`)

### Running

+ (use two separate terminal windows)
+ start MongoDB if you haven't already
+ start backend (again use virtualenv): `$ python -m lama.server --cors`
+ frontend needs a web server to work properly, for example: `$ (cd frontend/dist/ && python -m http.server)`
+ point your web browser at `http://localhost:8000` (if using the above command)

## Local development

+ build and setup are identical to local build, no need to compile JS though
+ start backend: `$ python -m lama.server --dev`
+ start frontend dev server: `$ (cd frontend/ && npm run start)`
+ (the dev server provides live reloading; on Linux, if you get an error about too many open files, you could try: `$ sysctl fs.inotify.max_user_watches=524288 && sysctl -p`)

## Additional notes for development

+ Whenever the word "Connection" appears in the code, it should probably be "Annotation".
+ Unfortunately, the code is not well-documented (sorry!); looking at the Props of React components and their type can really help.
+ If you're seriously going to do something with this, we suggest you reach out to the developers for support.


## User management

User data is stored in a separate SQLite DB (default: `lama_users.db`). Users can be managed through either a CLI-script (`scripts/lamaherder.py`) or the API (`/users`). However, if there are no existing users, an (probably admin) user will have to be created using the script.

    usage: lamaherder.py [-h] [--password [PASSWORD]] [--email EMAIL] [--privileges rwa] [--active yn]
                         [--reauth yn]
                         {showusers,createuser,deleteuser,updateuser} [username]

For example: `$ python3 scripts/lamaherder.py createuser admin --privileges a --password MyPass234`

(Usernames need to be 2-12 lowercase letters; passwords need to be 8-16 letters/numbers with at least one uppercase/lowercase/number.)

Once a user has been created, these credentials can be used to obtain an access token.

    $ curl -Ss -H 'Content-type: application/json' -d '{"username": "admin", "password": "MyPass234"}' <backend_url>/auth

Response:

    {
      "username": "admin",
      "privileges": "a",
      "token": "eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJleHAiOjE2NDAwOTg1MjAsInVzciI6InBwcm92YXpuaWsiLCJwcnYiOiJhIn0.KxIjvVYQpD_ffc6wfP-_dMXyPAMESYFlWeQlIOeq0DY",
      "expires": "2021-12-21T14:55:20+00:00"
    }

Then create users via API:

    $ curl -Ss -H 'Authorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJleHAiOjE2NDAwOTg1MjAsInVzciI6InBwcm92YXpuaWsiLCJwcnYiOiJhIn0.KxIjvVYQpD_ffc6wfP-_dMXyPAMESYFlWeQlIOeq0DY' -H 'Content-type: application/json' -X POST -d '{"username": "testuser"}' <backend_url>/users

Response:

    {"password": "RjJ4BBVtKqoq"}

If no password is set, it will be generated. See the `/users` routes in `server.py` for details.


### API Routes

See [routes.md](routes.md).
